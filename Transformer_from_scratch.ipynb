{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of parameters: 44.6638M\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 550\u001b[0m\n\u001b[1;32m    547\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m start_token\n\u001b[1;32m    549\u001b[0m config\u001b[38;5;241m=\u001b[39m Transformerconfig()\n\u001b[0;32m--> 550\u001b[0m model\u001b[38;5;241m=\u001b[39m \u001b[43mTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTransformerconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/govtscraper/lib/python3.10/site-packages/torch/nn/modules/module.py:1152\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/govtscraper/lib/python3.10/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/govtscraper/lib/python3.10/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/govtscraper/lib/python3.10/site-packages/torch/nn/modules/module.py:825\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 825\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    826\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
            "File \u001b[0;32m~/miniconda3/envs/govtscraper/lib/python3.10/site-packages/torch/nn/modules/module.py:1150\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math, copy, time\n",
        "from dataclasses import dataclass\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class MultiheadAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    A module that implements the multi-head self-attention mechanism\n",
        "    which is a key component of Transformer models.\n",
        "\n",
        "    The module takes a configuration object upon initialization which\n",
        "    should contain the necessary parameters such as the number of\n",
        "    heads (n_head), the embedding dimension (n_embd), the dropout rate\n",
        "    (dropout), and the use of bias.\n",
        "\n",
        "    Attributes:\n",
        "    c_attn (torch.nn.Module): A linear layer used to create query, key,\n",
        "        and value vectors from the input.\n",
        "    attn_dropout (torch.nn.Module): Dropout layer applied to attention weights.\n",
        "    n_head (int): The number of attention heads.\n",
        "    n_embd (int): The size of each embedding vector.\n",
        "    dropout (float): The dropout rate.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        \n",
        "        self.c_attn = nn.Linear(config.n_embd, 3*config.n_embd,bias=False)\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd,bias=False)\n",
        "\n",
        "        #regularization\n",
        "        self.attn_dropout= nn.Dropout(config.dropout)\n",
        "        self.resid_dropout= nn.Dropout(config.dropout)\n",
        "        self.n_head= config.n_head\n",
        "        self.n_embd= config.n_embd\n",
        "        self.dropout=config.dropout\n",
        "\n",
        "    def forward(self, x,src_mask=None, mask=None):\n",
        "        \"\"\"\n",
        "        Forward propagate the multi-head attention mechanism.\n",
        "\n",
        "        Parameters:\n",
        "        x (torch.Tensor): The input tensor of shape [batch size, sequence length, embedding size]\n",
        "        mask (torch.Tensor, optional): An optional mask tensor for the attention mechanism.\n",
        "\n",
        "        Returns:\n",
        "        y (torch.Tensor): The output of the multi-head attention of shape [batch size, sequence length, embedding size]\n",
        "        \"\"\"\n",
        "\n",
        "        B,T,C= x.size()\n",
        "\n",
        "        q,k,v= self.c_attn(x).split(self.n_embd,dim=2)\n",
        "        k= k.view(B,T, self.n_head, C//self.n_head).transpose(1,2) #(B, nh, T, hs)\n",
        "        q= q.view(B,T, self.n_head, C//self.n_head).transpose(1,2) #(B, nh, T, hs)\n",
        "        v= v.view(B,T, self.n_head, C//self.n_head).transpose(1,2) #(B, nh, T, hs)\n",
        "\n",
        "        att= (q@ k.transpose(-2,-1)) * (1.0/math.sqrt(k.size(-1)))\n",
        "        if mask!=None:\n",
        "            att= att.masked_fill(mask==0, float('-inf'))\n",
        "\n",
        "        if src_mask!=None:\n",
        "            att= att.masked_fill(src_mask[:, None, None, :] == 0, float(\"-inf\"))\n",
        "\n",
        "        att= F.softmax(att,dim=-1)\n",
        "        att=self.attn_dropout(att)\n",
        "\n",
        "        y=att@v #(B,nh,T,T) x (B,nh,T,hs) -> (B,nh,T,hs)\n",
        "\n",
        "        y=y.transpose(1,2).contiguous().view(B,T,C)\n",
        "\n",
        "        y= self.resid_dropout(self.c_proj(y))\n",
        "\n",
        "        return y\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class CrossMultiheadAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    A module that implements the multi-head self-attention mechanism\n",
        "    which is a key component of Transformer models.\n",
        "\n",
        "    The module takes a configuration object upon initialization which\n",
        "    should contain the necessary parameters such as the number of\n",
        "    heads (n_head), the embedding dimension (n_embd), the dropout rate\n",
        "    (dropout), and the use of bias.\n",
        "\n",
        "    Attributes:\n",
        "    c_attn (torch.nn.Module): A linear layer used to create query, key,\n",
        "        and value vectors from the input.\n",
        "    attn_dropout (torch.nn.Module): Dropout layer applied to attention weights.\n",
        "    n_head (int): The number of attention heads.\n",
        "    n_embd (int): The size of each embedding vector.\n",
        "    dropout (float): The dropout rate.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "\n",
        "        #Attention mechanism\n",
        "        self.c_attn= nn.Linear(config.n_embd, config.n_embd,bias=False)\n",
        "        self.c_attn2 = nn.Linear(config.n_embd, 2*config.n_embd,bias=False)\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd,bias=False)\n",
        "\n",
        "        #regularization\n",
        "        self.attn_dropout= nn.Dropout(config.dropout)\n",
        "        self.resid_dropout= nn.Dropout(config.dropout)\n",
        "        self.n_head= config.n_head\n",
        "        self.n_embd= config.n_embd\n",
        "        self.dropout=config.dropout\n",
        "\n",
        "    def forward(self, x, encoder_output, src_mask=None):\n",
        "        \"\"\"\n",
        "        Forward propagate the multi-head attention mechanism.\n",
        "\n",
        "        Parameters:\n",
        "        x (torch.Tensor): The input tensor of shape [batch size, sequence length, embedding size]\n",
        "        mask (torch.Tensor, optional): An optional mask tensor for the attention mechanism.\n",
        "\n",
        "        Returns:\n",
        "        y (torch.Tensor): The output of the multi-head attention of shape [batch size, sequence length, embedding size]\n",
        "        \"\"\"\n",
        "\n",
        "        B,T,C= x.size()\n",
        "\n",
        "        q= self.c_attn(x)\n",
        "        k,v= self.c_attn2(encoder_output).split(self.n_embd,dim=2)\n",
        "        k= k.view(B,T, self.n_head, C//self.n_head).transpose(1,2) #(B, nh, T, hs)\n",
        "        q= q.view(B,T, self.n_head, C//self.n_head).transpose(1,2) #(B, nh, T, hs)\n",
        "        v= v.view(B,T, self.n_head, C//self.n_head).transpose(1,2) #(B, nh, T, hs)\n",
        "        att= (q@ k.transpose(-2,-1)) * (1.0/math.sqrt(k.size(-1)))\n",
        "\n",
        "        if src_mask!=None:\n",
        "            att= att.masked_fill(src_mask[:, None, None, :] == 0, float(\"-inf\"))\n",
        "\n",
        "        att= F.softmax(att,dim=-1)\n",
        "        att=self.attn_dropout(att)\n",
        "        y=att@v #(B,nh,T,T) x (B,nh,T,hs) -> (B,nh,T,hs)\n",
        "        y=y.transpose(1,2).contiguous().view(B,T,C)\n",
        "\n",
        "        y= self.resid_dropout(self.c_proj(y))\n",
        "\n",
        "        return y\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class FeedForwardNeuralNetwork(nn.Module):\n",
        "  \"\"\"\n",
        "  A simple Feed-Forward Neural Network (FFNN) module consisting of two linear layers\n",
        "  with a GELU (Gaussian Error Linear Unit) activation function in between.\n",
        "\n",
        "  Note:\n",
        "  This FFNN is often used in the Transformer model after the Multihead Attention mechanism.\n",
        "\n",
        "  Attributes:\n",
        "  c_fc (torch.nn.Module): A linear layer used to transform the input data.\n",
        "  gelu (torch.nn.Module): An activation layer using GELU.\n",
        "  final_layer (torch.nn.Module): A final linear layer to transform\n",
        "      the data to the original dimensionality.\n",
        "  dropout (torch.nn.Module): Dropout layer applied after the final linear transformation.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.c_fc = nn.Linear(config.n_embd, 4*config.n_embd, bias=config.bias)\n",
        "    self.gelu = nn.GELU()\n",
        "    self.final_layer = nn.Linear(4*config.n_embd, config.n_embd, bias=config.bias)\n",
        "    self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    Forward propagate the FFNN.\n",
        "\n",
        "    Parameters:\n",
        "    x (torch.Tensor): The input tensor.\n",
        "\n",
        "    Returns:\n",
        "    x (torch.Tensor): The output of the FFNN.\n",
        "    \"\"\"\n",
        "    x = self.c_fc(x)\n",
        "    x = self.gelu(x)\n",
        "    x = self.final_layer(x)\n",
        "    x = self.dropout(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "  \"\"\"\n",
        "  A module to add positional encodings to the input sequences.\n",
        "\n",
        "  The positional encoding uses sinusoidal functions of different frequencies to\n",
        "  encode the position. It is added to the input embeddings to provide\n",
        "  information about the relative position of the words in a sentence.\n",
        "\n",
        "  Attributes:\n",
        "  pe (torch.Tensor): The positional encodings for maximum sequence length.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, config, t=256):\n",
        "    super().__init__()\n",
        "\n",
        "    pe = torch.zeros(t, config.n_embd)\n",
        "    position= torch.arange(0, t, dtype=torch.float).unsqueeze(1)\n",
        "    div_term= torch.exp(torch.arange(0, config.n_embd, 2).float() * -(math.log(10000.0) / config.n_embd))\n",
        "    pe[:, 0::2] = torch.sin(position * div_term)\n",
        "    pe[:, 1::2] = torch.cos(position * div_term)\n",
        "    self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "  def forward(self,x):\n",
        "    \"\"\"\n",
        "    Forward propagate the positional encodings.\n",
        "\n",
        "    Parameters:\n",
        "    x (torch.Tensor): The input tensor.\n",
        "\n",
        "    Returns:\n",
        "    (torch.Tensor): The input tensor added with positional encodings.\n",
        "    \"\"\"\n",
        "    return x + self.pe[:, :x.size(1)]\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "  \"\"\"\n",
        "  A module for Layer Normalization: normalization is done across\n",
        "  each feature in a training example. As compared to Batch\n",
        "  Normalization, Layer Normalization is not sensitive to the batch\n",
        "  size.\n",
        "\n",
        "  Attributes:\n",
        "  weight (torch.nn.Parameter): learnable scale factors. The number of\n",
        "    these factors is equal to the input size.\n",
        "  bias (torch.nn.Parameter): learnable shift factors. The number of\n",
        "    these factors is equal to the input size.\n",
        "  \"\"\"\n",
        "  def __init__(self, ndim, bias):\n",
        "    super().__init__()\n",
        "    # Initialize the learnable parameters weight and bias.\n",
        "    self.weight = nn.Parameter(torch.ones(ndim))\n",
        "    self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
        "\n",
        "  def forward(self, input):\n",
        "    \"\"\"\n",
        "    Forward propagate the LayerNorm module.\n",
        "\n",
        "    Parameters:\n",
        "    input (torch.Tensor): The input tensor.\n",
        "\n",
        "    Returns:\n",
        "    (torch.Tensor): The output of the Layer Normalization.\n",
        "    \"\"\"\n",
        "    return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "  \"\"\"\n",
        "  A transformer encoder module. Consists of a Multihead Attention mechanism\n",
        "  followed by positional feedforward neural network. Layer normalization is\n",
        "  applied before the attention and feedforward network, respectively\n",
        "\n",
        "  Attributes:\n",
        "  ln_1 (LayerNorm): The first layernorm module, applied before attention mechanism\n",
        "  attn (MultiheadAttention): The attention mechanism\n",
        "  ln_2 (LayerNorm): The second layernorm module, applied before feedforward network\n",
        "  FeedForwardNeuralNetwork (FeedForwardNeuralNetwork): The positional feedforward network\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self,config):\n",
        "    super().__init__()\n",
        "    self.ln_1= LayerNorm(config.n_embd,bias=config.bias)\n",
        "    self.attn= MultiheadAttention(config)\n",
        "    self.ln_2 = LayerNorm(config.n_embd, bias= config.bias)\n",
        "    self.mlp= FeedForwardNeuralNetwork(config)\n",
        "\n",
        "  def forward(self,x,src_mask):\n",
        "    \"\"\"\n",
        "    Forward propagate the encoder module.\n",
        "\n",
        "    Parameters:\n",
        "    x (torch.Tensor): The input tensor.\n",
        "\n",
        "    Returns:\n",
        "    x (torch.Tensor): The output of the encoder module.\n",
        "    \"\"\"\n",
        "    x = x+self.attn(self.ln_1(x),src_mask)\n",
        "    x= x+ self.mlp(self.ln_2(x))\n",
        "    return x\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Transformerconfig:\n",
        "  block_size: int =256\n",
        "  vocab_size: int = 194\n",
        "  n_layer: int= 6\n",
        "  n_head: int =8\n",
        "  n_embd: int= 512\n",
        "  dropout: float=0.0\n",
        "  bias: bool = True\n",
        "  padding_idx: int = 0\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "  \"\"\"\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self,config):\n",
        "    super().__init__()\n",
        "    self.ln_1= LayerNorm(config.n_embd,bias=config.bias)\n",
        "    self.attn_1= MultiheadAttention(config)\n",
        "    self.ln_2 = LayerNorm(config.n_embd,bias= config.bias)\n",
        "    self.attn_2= CrossMultiheadAttention(config)\n",
        "    self.ln_3 = LayerNorm(config.n_embd, bias= config.bias)\n",
        "    self.mlp= FeedForwardNeuralNetwork(config)\n",
        "\n",
        "  def forward(self,x,src_mask,mask,encoder_output):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    x = self.attn_1(self.ln_1(x), src_mask, mask)\n",
        "    x = x+ self.attn_2(self.ln_2(x),encoder_output,src_mask)\n",
        "    x= x+ self.mlp(self.ln_3(x))\n",
        "    return x\n",
        "  \n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "  def __init__(self,config):\n",
        "    super().__init__()\n",
        "    assert config.vocab_size is not None\n",
        "    assert config.block_size is not None\n",
        "    self.config= config\n",
        "\n",
        "\n",
        "    self.transformer_encoder= nn.ModuleDict(dict(\n",
        "        wte= nn.Embedding(config.vocab_size,config.n_embd,padding_idx=config.padding_idx),\n",
        "        wpe= nn.Embedding(config.block_size,config.n_embd,padding_idx=config.padding_idx),\n",
        "        # wpe= PositionalEncoding(config),\n",
        "        drop= nn.Dropout(config.dropout),\n",
        "        h= nn.ModuleList([Encoder(config) for _ in range(config.n_layer)]),\n",
        "        ln_f= LayerNorm(config.n_embd, bias=config.bias)\n",
        "    ))\n",
        "\n",
        "    self.transformer_decoder= nn.ModuleDict(dict(\n",
        "        wte= nn.Embedding(config.vocab_size,config.n_embd,padding_idx=config.padding_idx),\n",
        "        wpe= nn.Embedding(config.block_size,config.n_embd,padding_idx=config.padding_idx),\n",
        "        # wpe= PositionalEncoding(config),\n",
        "        drop= nn.Dropout(config.dropout),\n",
        "        h= nn.ModuleList([Decoder(config) for _ in range(config.n_layer)]),\n",
        "        ln_f= LayerNorm(config.n_embd, bias=config.bias)\n",
        "    ))\n",
        "\n",
        "    self.lm_head= nn.Linear(config.n_embd, config.vocab_size,bias=False)\n",
        "\n",
        "    self.apply(self._init_weights)\n",
        "\n",
        "    for pn,p in self.named_parameters():\n",
        "        if pn.endswith('c_proj.weight'):\n",
        "            torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2*config.n_layer))\n",
        "\n",
        "\n",
        "    print(\"number of parameters: %.4fM\" % (self.get_num_params()/1e6,))\n",
        "\n",
        "  \n",
        "  \n",
        "  def get_num_params(self, non_embedding=False):\n",
        "        \"\"\"\n",
        "        Return the number of parameters in the model.\n",
        "        For non-embedding count (default), the position embeddings get subtracted.\n",
        "        The token embeddings would too, except due to the parameter sharing these\n",
        "        params are actually used as weights in the final layer, so we include them.\n",
        "        \"\"\"\n",
        "        n_params = sum(p.numel() for p in self.parameters())\n",
        "        if non_embedding:\n",
        "            n_params -= self.transformer_encoder.wpe.weight.numel()+self.transformer_decoder.wpe.weight.numel()\n",
        "        return n_params\n",
        "  \n",
        "\n",
        "  \n",
        "  \n",
        "  def _init_weights(self, module):\n",
        "      if isinstance(module, nn.Linear):\n",
        "          torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "          if module.bias is not None:\n",
        "              torch.nn.init.zeros_(module.bias)\n",
        "      elif isinstance(module, nn.Embedding):\n",
        "          torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "  \n",
        "  \n",
        "  \n",
        "  def forward(self, idx,target_idx,src_mask=None,tgt_mask=None, mask=None, targets=None):\n",
        "\n",
        "    device= idx.device\n",
        "\n",
        "    # targets= target_idx\n",
        "\n",
        "    b,t= idx.size()\n",
        "    # self.config.block_size= t\n",
        "\n",
        "    assert t<= self.config.block_size,f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
        "    pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
        "    #Encoder part Starts\n",
        "    tok_emb= self.transformer_encoder.wte(idx)\n",
        "    pos_emb = self.transformer_encoder.wpe(pos) \n",
        "    x = self.transformer_encoder.drop(tok_emb + pos_emb)\n",
        "    for block in self.transformer_encoder.h:\n",
        "      encoder_output = block(x,src_mask)\n",
        "    encoder_output= self.transformer_encoder.ln_f(encoder_output)\n",
        "\n",
        "\n",
        "    b,t= target_idx.size()\n",
        "    pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
        "\n",
        "    #Decoder part Starts\n",
        "    out_tok_emb= self.transformer_decoder.wte(target_idx)\n",
        "    print(f\"out_tok_emb: {out_tok_emb.size()}\")\n",
        "    pos_emb = self.transformer_decoder.wpe(pos)\n",
        "    print(f\"positional_emb: {pos_emb.size()}\")\n",
        "    x_decoder = self.transformer_decoder.drop(out_tok_emb + pos_emb)\n",
        "    print(f\"x_decoder: {x_decoder.size()}\")\n",
        "    for block in self.transformer_decoder.h:\n",
        "      x_decoder= block(x_decoder,tgt_mask,mask,encoder_output)\n",
        "    x_decoder= self.transformer_decoder.ln_f(x_decoder)\n",
        "  \n",
        "    \n",
        "    if targets is not None:\n",
        "            targets= target_idx\n",
        "            logits = self.lm_head(x_decoder)\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=0)\n",
        "    else:\n",
        "            logits = self.lm_head(x_decoder[:, [-1], :]) \n",
        "            loss = None\n",
        "\n",
        "    return logits, loss\n",
        "  \n",
        "\n",
        "  def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n",
        "\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters() if p.requires_grad}\n",
        "\n",
        "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
        "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
        "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
        "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
        "        \n",
        "        optim_groups = [\n",
        "            {'params': decay_params, 'weight_decay': weight_decay},\n",
        "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
        "        ]\n",
        "        \n",
        "        num_decay_params = sum(p.numel() for p in decay_params)\n",
        "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
        "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
        "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
        "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas)\n",
        "        return optimizer\n",
        "  \n",
        "  @torch.no_grad()\n",
        "  def generate(self, idx, start_token, eos_token, temperature=1.0, device=None):\n",
        "    \"\"\"\n",
        "    Generate sequences from the model.\n",
        "\n",
        "    Parameters:\n",
        "    idx (torch.Tensor): The input tensor.\n",
        "    start_token (int): The token to start the generation with.\n",
        "    eos_token (int): The token representing the end of the sequence.\n",
        "    temperature (float): A temperature value to apply to the logits before sampling.\n",
        "    device (torch.device): The device to run the model on.\n",
        "\n",
        "    Returns:\n",
        "    (torch.Tensor): The generated sequence.\n",
        "    \"\"\"\n",
        "    self.eval()\n",
        "\n",
        "    src_mask= idx!=0\n",
        "\n",
        "    if device is None:\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    idx = idx.to(device)\n",
        "    _, t = idx.size()\n",
        "    for i in range(256):\n",
        "        logits, _ = self(idx,start_token,src_mask)\n",
        "        logits = logits[:, -1, :] / temperature\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        next_token = torch.argmax(probs, dim=-1)\n",
        "        print(i,next_token)\n",
        "\n",
        "        if (next_token == eos_token).any():\n",
        "            break \n",
        "\n",
        "        start_token = torch.cat((start_token, next_token), dim=-1)\n",
        "        print(start_token)\n",
        "\n",
        "\n",
        "    return start_token\n",
        "  \n",
        "config= Transformerconfig()\n",
        "model= Transformer(Transformerconfig()).to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pad_token = 0\n",
        "max_length = config.block_size\n",
        "src_data= torch.randint(1, config.vocab_size, (16,max_length)).to(device)\n",
        "\n",
        "# Create a random length for padding for each instance in the batch\n",
        "lengths = torch.randint(1, max_length, (16,)).to(device)\n",
        "\n",
        "# Create a mask for each instance in the batch use it to insert padding tokens in the src_data\n",
        "mask = torch.arange(max_length).expand(16, max_length).to(device) < lengths.unsqueeze(1)\n",
        "src_data = src_data.where(mask, pad_token)\n",
        "\n",
        "src_mask=src_data!= 0\n",
        "\n",
        "tgt_data = torch.randint(1, config.vocab_size, (16,max_length)).to(device)\n",
        "\n",
        "tgt_mask= tgt_data!=0   \n",
        "\n",
        "nopeak_mask = (1 - torch.triu(torch.ones(1, max_length, max_length), diagonal=1)).bool().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "out_tok_emb: torch.Size([16, 256, 512])\n",
            "positional_emb: torch.Size([256, 512])\n",
            "x_decoder: torch.Size([16, 256, 512])\n",
            "torch.Size([16, 256, 194]) tensor(5.3636, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ],
      "source": [
        "logits,loss= model(src_data,tgt_data,src_mask,tgt_mask,nopeak_mask,1)\n",
        "print(logits.shape, loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1]])"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.tensor([1]).unsqueeze(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "data= pd.read_csv(\"/mnt/sparsh_transformers/transliteration_data/hi.romanized.rejoined.aligned.cased_nopunct.tsv\",sep=\"\\t\",header=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>जबकि</td>\n",
              "      <td>jabki</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>यह</td>\n",
              "      <td>yah</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>जैनों</td>\n",
              "      <td>jainon</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>से</td>\n",
              "      <td>se</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>कम</td>\n",
              "      <td>km</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>186516</th>\n",
              "      <td>असहयोग</td>\n",
              "      <td>asahayog</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>186517</th>\n",
              "      <td>आंदोलन</td>\n",
              "      <td>andolan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>186518</th>\n",
              "      <td>शुरू</td>\n",
              "      <td>shuru</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>186519</th>\n",
              "      <td>किया</td>\n",
              "      <td>kiya</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>186520</th>\n",
              "      <td>&lt;/s&gt;</td>\n",
              "      <td>&lt;/s&gt;</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>186521 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             0         1\n",
              "0         जबकि     jabki\n",
              "1           यह       yah\n",
              "2        जैनों    jainon\n",
              "3           से        se\n",
              "4           कम        km\n",
              "...        ...       ...\n",
              "186516  असहयोग  asahayog\n",
              "186517  आंदोलन   andolan\n",
              "186518    शुरू     shuru\n",
              "186519    किया      kiya\n",
              "186520    </s>      </s>\n",
              "\n",
              "[186521 rows x 2 columns]"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = data[~data[0].str.contains('</s>')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_duplicated= data.drop_duplicates([0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>जबकि</td>\n",
              "      <td>jabki</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>यह</td>\n",
              "      <td>yah</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>जैनों</td>\n",
              "      <td>jainon</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>से</td>\n",
              "      <td>se</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>कम</td>\n",
              "      <td>km</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>186420</th>\n",
              "      <td>फिल्माना</td>\n",
              "      <td>filmana</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>186434</th>\n",
              "      <td>फिल्माया</td>\n",
              "      <td>filmaya</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>186460</th>\n",
              "      <td>बोली-क्षेत्रों</td>\n",
              "      <td>boli-kshetro</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>186470</th>\n",
              "      <td>रेख</td>\n",
              "      <td>rekh</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>186479</th>\n",
              "      <td>सिलाई</td>\n",
              "      <td>silai</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>24868 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                     0             1\n",
              "0                 जबकि         jabki\n",
              "1                   यह           yah\n",
              "2                जैनों        jainon\n",
              "3                   से            se\n",
              "4                   कम            km\n",
              "...                ...           ...\n",
              "186420        फिल्माना       filmana\n",
              "186434        फिल्माया       filmaya\n",
              "186460  बोली-क्षेत्रों  boli-kshetro\n",
              "186470             रेख          rekh\n",
              "186479           सिलाई         silai\n",
              "\n",
              "[24868 rows x 2 columns]"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_duplicated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_duplicated[0].to_csv('hi.txt',index=False,header=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_duplicated[1].to_csv('transliterated.txt',index=False,header=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('transliterated.txt') as f:\n",
        "    lines= f.read().splitlines()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "39"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "max_length=0\n",
        "for i in lines:\n",
        "    max_length= max(max_length,len(i))\n",
        "\n",
        "max_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
